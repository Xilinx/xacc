- title: "AuroraFlow, an Easy-to-Use, Low-Latency FPGA Communication Solution Demonstrated on Multi-FPGA Neural Network Inference"
  author: "Gerrit Pape and Bjarne Wintermann"
  institution: "Paderborn University"
  link: ""
  abstract: |
    "Communication between FPGAs has become increasingly important in recent years, with a multitude of implementations available. In this work, we focus specifically on direct FPGA-to-FPGA connections with a serial protocol. We present AuroraFlow−which is built on top of the official IP for the Aurora protocol from AMD−and use it to implement multi-FPGA neural network inference. AuroraFlow makes it easy to integrate low-latency communication into existing FPGA applications using High-Level Synthesis (HLS). It achieves full throughput, has integrated monitoring to detect errors at run-time, and implements flow control, which prevents data loss caused by receiver overruns. In a circuit-switched optical network, we are able to achieve an average latency of down to 0.51 µs and an average throughput of up to 95.03 Gbit/s for a message size of 1 MiB. To demonstrate AuroraFlow beyond microbenchmarks, we execute distributed neuronal network inference on multiple data center FPGAs. For this, we use FINN, a framework for creating dataflow hardware designs for neural network inference. We extend FINN by integrating AuroraFlow to work with multiple FPGAs. We showcase the commonly referenced MobileNet network architecture as an example where both tools can be combined: While FINN takes care of creating the inference design itself, AuroraFlow manages the communication between the FPGAs. Using AuroraFlow gives us the possibility to keep the benefit of very low inference latency that FINN enables, while allowing us to scale beyond a single FPGA."

- title: "A Unified Framework for Automated Code Transformation and Pragma Insertion"
  author: "Stéphane Pouget"
  institution: "UCLA"
  link: "https://dl.acm.org/doi/10.1145/3706628.3708873"
  abstract: |
    "High-Level Synthesis compilers and Design Space Exploration tools have greatly advanced the automation of hardware design, improving development time and performance. However, achieving a good Quality of Results still requires extensive manual code transformations, pragma insertion, and tile size selection, which are typically handled separately. The design space is too large to be fully explored by this fragmented approach. It is too difficult to navigate this way, limits the exploration of potential optimizations, and complicates the design generation process.
    To tackle this obstacle, we propose Sisyphus, a unified framework that automates code transformation, pragma insertion, and tile size selection within a common optimization framework. By leveraging Nonlinear Programming, our approach efficiently explores the vast design space of regular loop-based kernels, automatically selecting loop transformations and pragmas that minimize latency.
    Evaluation against state-of-the-art frameworks, including AutoDSE, NLP-DSE, and ScaleHLS, shows that Sisyphus achieves superior Quality of Results, outperforming alternatives across multiple benchmarks. By integrating code transformation and pragma insertion into a unified model, Sisyphus significantly reduces design generation complexity and improves performance for FPGA-based systems."

- title: "BPAP: FPGA Design of a RISC-like Processor for Elliptic Curve Cryptography Using Task-Level Parallel Programming in High-Level Synthesis"
  author: "Rares Ifrim"
  institution: "National University of Science and Technology Politehnica Bucharest"
  link: "https://www.mdpi.com/2410-387X/9/1/20"
  github: "https://gitlab.com/raresifrim/secp256k1_ecc_pointmul"
  abstract: |
    "Popular technologies such as blockchain and zero-knowledge proof, which have already entered the enterprise space, heavily use cryptography as the core of their protocol stack. One of the most used systems in this regard is Elliptic Curve Cryptography, precisely the point multiplication operation, which provides the security assumption for all applications that use this system. As this operation is computationally intensive, one solution is to offload it to specialized accelerators to provide better throughput and increased efficiency. In this paper, we explore the use of Field Programmable Gate Arrays (FPGAs) and the High-Level Synthesis framework of AMD Vitis in designing an elliptic curve point arithmetic unit (point adder) for the secp256k1 curve. We show how task-level parallel programming and data streaming are used in designing a RISC processor-like architecture to provide pipeline parallelism and increase the throughput of the point adder unit. We also show how to efficiently use the proposed processor architecture by designing a point multiplication scheduler capable of scheduling multiple batches of elliptic curve points to utilize the point adder unit efficiently. Finally, we evaluate our design on an AMD-Xilinx Alveo-family FPGA and show that our point arithmetic processor has better throughput and frequency than related work."

- title: "Coyote v2: Towards Open-Source, Reusable Infrastructure and Abstractions for FPGAs"
  author: "Benjamin Ramhorst and Maximilian J. Heer"
  institution: "ETH Zurich"
  link: "https://capra.cs.cornell.edu/latte25/paper/3.pdf"
  abstract: |
    "We identify a number of requirements for future FPGA shells and describe Coyote v2, our first step towards a unified FPGA platform for cloud and datacenter acceleration, providing support similar to a conventional OS on a CPU"

- title: "Efficient and Distributed Computation of Electron Repulsion Integrals on AMD AI Engines"
  author: "Johannes Menzel and Christian Plessl"
  institution: "Paderborn University"
  link: ""
  abstract: |
    "Computing electron repulsion integrals (ERIs) is the major computational bottleneck of many quantum mechanical simulation methods, requiring trillions of ERI evaluations per time step. While the computation of independent ERIs is embarrassingly parallel, the efficient computation of individual ERIs on modern processor cores is difficult due to both an insufficient cache size for intermediates of the computation and irregular memory access patterns that are difficult to vectorize. In this paper, we present how our implementation on the AI Engine (AIE) architecture addresses both of these problems. First, we have defined a flexible graph structure, which we call an ERI-Engine, that can be implemented for all 231 canonical ERI quartets from {ss|ss} to {hh|hh} by distributing the computation over 2-14 AIEs. Second, for the larger quartets, we have devised a novel vectorization scheme that leverages the advanced floating-point unit of the AIEs, while also supporting vectorization of independent ERIs for the smaller quartets. Finally, ERI-Engines are horizontally and vertically stackable to fill the entire AIE array, and in particular, the vertically stacked ERI-Engines form a column that uses one or more time-shared channels to stream the results out of the AIE array, almost completely hiding the computational phases of individual ERI-Engines. In terms of absolute performance, we are competitive with recent high-performance implementations of ERI algorithms on FPGAs (SERI) and GPUs (LibintX), as well as well-established highly optimized CPU libraries (Libint, Libcint), while being the unequivocal leader in terms of energy efficiency."

- title: "Evaluating the Strong Scaling Potential of AI Engines for Molecular Dynamics Simulations"
  author: "Mika Bröker"
  institution: "Paderborn University"
  link: ""
  abstract: |
    "In molecular dynamics (MD) applications, there is a constant need to simulate ever longer timescales for a fixed number of particles. Here, novel spatial architectures such as the AI Engines (AIEs), with their low-latency interconnect between computational units, are promising candidates for breaking this strong scaling barrier. In this work, we have prototyped an MD simulation on the AIEs using cell lists that subdivide and map the simulation volume to AIEs for local computation. Between time steps, particles must be exchanged globally, for which we leveraged the on-chip switch network. We initially implemented this global communication using a ring topology, but a thorough evaluation revealed that this hindered strong scaling. Therefore, we are currently in the process of changing the global communication to a packet-switching and routing scheme to allow more direct particle exchange between AIEs. Here, we report first results and challenges posed by the architecture for an efficient implementation."

- title: "FINN-HPC: Closing the Gap for Energy-Efficient Neural Network Inference on FPGAs in HPC"
  author: "Linus Jungemann"
  institution: "Paderborn University"
  link: ""
  abstract: |
    "In recent years, neural network (NN) training and inference have become one of the most impactful topics in society, research and industry. With the rise in popularity of NNs and the increase in their capabilities and size, new challenges surface, such as a high energy consumption. This paper presents a comprehensive evaluation of current NN inference soft- and hardware configurations within High-Performance Computing (HPC) environments, with a focus on both performance metrics and energy consumption. NN quantization and the use of accelerators such as FPGAs can improve throughput and energy efficiency. This paper focuses on FINN, an efficient NN inference framework for FPGAs, highlighting its current lack of support for HPC systems. We provide an in-depth analysis of FINN in order to implement extensions and a new user-space driver for FINN to optimize the end-to-end execution for the usage in the HPC environment. We thoroughly evaluate the performance and energy efficiency gains using the newly implemented optimizations for a NN requiring low latency and high throughput, as well as for a transformer model. The results are compared against existing NN accelerators for HPC. Our results show that with our newly developed FINN HPC driver, FPGAs have a real use case depending on input sizes and network. For the simpler model, we show that for smaller input sizes, FPGAs with our driver can provide up to 7.81× higher throughput and 88% lower energy consumption over the GPU reference, which however still performs better for larger input sizes. Additionally, we evaluate the newly implemented FINN HPC driver on the significantly more complex transformer based model. We show that for smaller input sizes, our optimizations results in an up 6.51× higher throughput, while at the same time reducing energy consumption by 87% and latency by 85%."

- title: "LLM Acceleration on FPGAs: A Comparative Study of Layer and Spatial Accelerators"
  author: "Luis D. Prieto-Sibaja"
  institution: "Costa Rica Institute of Technology"
  link: "https://ieeexplore.ieee.org/document/10933896"
  abstract: |
    "Large Language Models (LLMs) have emerged as the leading technique in Natural Language Processing due to their remarkable capabilities. These models are exceptionally complex, often utilising billions of parameters and consuming tens of gigabytes of memory unless optimised. Typically, LLMs are trained and accelerated using general-purpose Graphics Processing Units (GPUs), with a single inference potentially occupying an entire GPU. Consequently, cloud services may require hundreds of GPUs to deliver high-quality AI assistant services. This demand for extensive hardware acceleration allows exploring alternative architectures that offer improved execution time and energy efficiency.This study evaluates Field-Programmable Gate Arrays (FP-GAs), renowned for their flexibility and efficiency, to accelerate LLMs. We specifically compare the resource consumption and latency of two distinct architectures: layer and spatial accelerators. Analysing the Llama 2-7B model, we identified potential optimisation opportunities within its composition and operational graphs. Our most successful implementations demonstrate a performance improvement ranging from 1.37× to 10.98× over two AMD EPYC processors with 64 cores each. Moreover, our results indicate that, with further refinement, FPGAs have the potential to surpass GPU performance for LLM inference, showcasing their feasibility as a viable alternative for this demanding application."


- title: "RoCE BALBOA: Towards FPGA-enhanced RDMA"
  author: "Maximilian J. Heer"
  institution: "ETH Zurich"
  link: "https://2025.eurosys.org/posters/final/eurosys25posters-final84.pdf"
  abstract: |
    "We explore the use of FPGA-based SmartNICs in the context of capability-enhanced RDMA, by combining a self-developed, open-source and fully RoCE v2-compatible networking stack with exemplaric hardware modules for AESencryption, snappy-compression, and machine learning-based Deep Packet Inspection (DPI) for access control that utilize stream computing to achieve 100G line rate speed without any performance overhead on the host CPU (Figure 1). Furthermore, our design allows to add any other user-defined application directly on the RDMA data streams, leveraging the reconfigurable fabric for offloading network processing from the host CPU to the FPGA-NIC at full line rate."

- title: "Rock the QASBA: Quantum Error Correction Acceleration via the Sparse Blossom Algorithm on FPGAs"
  author: "Marco Venere"
  institution: "Politecnico di Milano"
  link: ""
  abstract: |
    "Quantum computing is a new paradigm of computation that exploits principles from quantum mechanics to achieve an exponential speedup compared to classical logic. However, noise strongly limits current quantum hardware, reducing achievable performance and limiting the scaling of the applications. For this reason, current noisy intermediate-scale quantum devices requireQuantum Error Correction (QEC) mechanisms to identify errors occurring in the computation and correct them in real time. Nevertheless, the high computational complexity of QEC algorithms is incompatible with the tight time constraints of quantum devices. Thus, hardware acceleration is paramount to achieving real-time QEC. This work presents QASBA, an FPGA-based hardware accelerator for the Sparse Blossom Algorithm (SBA), a state-of-the-art decoding algorithm. After profiling the state-of-the-art software counterpart, we developed a design methodology for hardware development based on the SBA. We also devised an automation process to help users without expertise in hardware design in deploying architectures based on QASBA. We implement QASBA on different FPGA architectures and experimentally evaluate resource usage, execution time, and energy efficiency of our solution. Our solution attains up to 25.05× speedup and 304.16× improvement in energy efficiency compared to the software baseline."

- title: "SAT-Accel: A Modern SAT Solver on a FPGA"
  author: "Michael Lo"
  institution: "UCLA"
  link: "https://dl.acm.org/doi/10.1145/3706628.3708869"
  abstract: |
    "Boolean satisfiability (SAT) solving is the first known NP-complete problem and is widely used in many application domains. Over the years, there have been so many consistent improvements in this area such that larger instances can be solved relatively quickly. Although these improvements have found their way onto CPU implementations, there has been limited progress adopting this on hardware accelerators mainly because it is difficult to implement the dynamic data structures needed to support a modern SAT solving algorithm. In this work, we present SAT-Accel, an algorithm-hardware co-design solver that applies many of the core improvements found in modern SAT solvers. SAT-Accel uses a novel memory management system and representation that supports the dynamic data structures required by a modern SAT solving algorithm. Our design can achieve on average a 17.9x speedup against MiniSat, the previous state of the art CPU solver, and a 2.8x speedup against Kissat, the current state of the art CPU solver. Compared to the current state-of-the-art stand-alone hardware accelerator, SAT-Hard, SAT-Accel achieves on average 800.0x speedup"

- title: "Stream-HLS: Towards Automatic Dataflow Acceleration"
  author: "Suhail Basalama"
  institution: "UCLA"
  link: "https://dl.acm.org/doi/10.1145/3706628.3708878"
  github: "https://github.com/UCLA-VAST/Stream-HLS"
  abstract: |
    "High-level synthesis (HLS) has enabled the rapid development of custom hardware circuits for many software applications. However, developing high-performance hardware circuits using HLS is still a non-trivial task requiring expertise in hardware design. Further, the hardware design space, especially for multi-kernel applications, grows exponentially. Therefore, several HLS automation and abstraction frameworks have been proposed recently, but many issues remain unresolved. These issues include: 1) relying mainly on hardware directives (pragmas) to apply hardware optimizations without exploring loop scheduling opportunities. 2) targeting single-kernel applications only. 3) lacking automatic and/or global design space exploration. 4) missing critical hardware optimizations, such as graph-level pipelining for multi-kernel applications.
    To address these challenges, we propose a novel methodology and framework on top of the popular multi-level intermediate representation (MLIR) infrastructure called Stream-HLS. Our framework takes a C/C++ or PyTorch software code and automatically generates an optimized dataflow architecture along with host code for field-programmable gate arrays (FPGAs). To achieve this, we developed an accurate analytical performance model for global scheduling and optimization of dataflow architectures. Stream-HLS is evaluated using various standard HLS benchmarks and real-world benchmarks from transformer models, convolution neural networks, and multilayer perceptrons. Stream-HLS designs outperform the designs of prior state-of-the-art automation frameworks and manually-optimized designs of abstraction frameworks by up to 79.43× and 10.62× geometric means respectively"

- title: "VOTED: Versal optimization toolkit for education and heterogeneous systems development"
  author: "Giuseppe Sorrentino"
  institution: "Politecnico di Milano"
  link: ""
  github: "https://github.com/necst/voted"
  abstract: |
    "Despite classic educational approaches proving their effectiveness for classic hardware acceleration, they suffer novel heterogeneous systems such as Versal, requiring a deeper system-level awareness. Versal devices merge FPGA on-field programmability with the performance of hardened VLIW processors, namely AI Engine, at the cost of facing novel challenges when integrating these two different layers. Given the system complexity of such devices and the low-level knowledge required to leverage them, Versal potentialities have yet to be fully exploited. Therefore, we present VOTED, a Versal Optimization Toolkit for Education and Heterogeneous Systems Development, that guides users of any expertise in discovering, using, and optimizing Versal-based applications. VOTED proved to be effective, leading five different groups of students, at their first experience with heterogeneous system design, at devising quite complex Versal-based applications capable of reaching the final stages of a design competition and even winning it with four months of work"

- title: "Clementi: Efficient Load Balancing and Communication Overlap for Multi-FPGA Graph Processing"
  author: "Feng Yu"
  institution: "National University of Singapore"
  link: ""
  github: "https://github.com/Xtra-Computing/Clementi"
  abstract: |
    "Efficient graph processing is critical in various modern applications, such as social network analysis, recommendation systems, and large-scale data mining. Traditional single-FPGA systems struggle to handle the increasing size and complexity of real-world graphs due to limitations in memory and computational resources. Existing multi-FPGA solutions face significant challenges, including high communication overhead caused by irregular data transfer patterns and workload imbalances stemming from skewed graph distributions. These inefficiencies hinder scalability and performance, highlighting a critical research gap. To address these issues, we introduce Clementi, an efficient multi-FPGA graph processing framework that features customized fine-grained pipelines for computation and cross-FPGA communication. Clementi uniquely integrates an accurate performance model for execution time prediction, enabling a novel scheduling method that balances workload distribution and minimizes communication overhead by overlapping communication and computation stages. Experimental results demonstrate that Clementi achieves speedups of up to 8.75x compared to state-of-the-art multi-FPGA designs, indicating significant improvements in processing efficiency as the number of FPGAs increases. This near-linear scalability underscores the framework's potential to enhance graph processing capabilities in practical applications."

- title: "Configurable DSP-Based CAM Architecture for Data-Intensive Applications on FPGAs"
  author: "Yao Chen"
  institution: "National University of Singapore"
  link: ""
  github: "https://github.com/Xtra-Computing/DSP_CAM"
  abstract: |
    "Content-addressable memory (CAM) is a type of fast memory unique in its ability to perform parallel searches of stored data based on content rather than specific memory addresses. They have been used in many domains, such as networking, databases, and graph processing. Field-programmable gate arrays (FPGAs) are an attractive platform for implementing CAMs because of their low latency, reconfigurability, and energy-efficient nature. However, such implementations also face significant challenges, including high resource utilization, limited scalability, and suboptimal performance due to the extensive use of look-up tables (LUTs) and block RAMs (BRAMs). These issues stem from the inherent limitations of FPGA architectures when handling the parallel operations required by CAMs, often leading to inefficient designs that cannot meet the demands of high-speed, data-intensive applications. To address these challenges, we propose a novel configurable CAM architecture that leverages the digital signal processing (DSP) blocks available in modern FPGAs as the core resource. By utilizing DSP blocks' data storage and logic capabilities, our approach enables configurable CAM architecture with efficient multi-query support while significantly reducing search and update latency for data-intensive applications. The DSP-based CAM architecture offers enhanced scalability, higher operating frequency, and improved performance compared to traditional LUT and BRAM-based designs. In addition, we demonstrate the effectiveness of our proposed CAM architecture with a triangle counting application on real graphs. This innovative use of DSP blocks also opens up new possibilities for high-performance, data-intensive applications on FPGAs."

- title: "Leda: Leveraging Tiling Dataflow to Accelerate SpMM on HBM-Equipped FPGAs for GNNs"
  author: "Enxin Yi"
  institution: "China University of Petroleum-Beijing"
  link: "https://dl.acm.org/doi/10.1145/3676536.3676773"
  github: ""
  abstract: |
    "Graph neural networks (GNNs) play a pivotal role in extracting insightful representations from graph-structured data, driving advancements across diverse domains. Central to GNNs is the sparse matrix-dense matrix multiplication (SpMM) kernel. However, challenges arise in accelerating SpMM due to the high sparsity and randomly distributed non-zeros in graph matrices. Recently, the high concurrency capability of high bandwidth memory (HBM) has provided a new opportunity for SpMM acceleration. Nonetheless, accelerating SpMM on HBM FPGAs is still non-trivial due to load imbalance and the random memory access patterns.
    In this paper, we present Leda, a high-performance SpMM accelerator on HBM-equipped FPGAs for GNNs. Leda utilizes a tiling sparse format to balance the workload between processing engines (PEs) and enhance data locality. The improved outer-product dataflow scheduling strategy mitigates the random memory access bottlenecks. Furthermore, we introduce a minimum similarity reordering algorithm to significantly optimize read-after-write (RAW) dependencies, thereby improving compute occupancy and throughput. Finally, we propose a highly parallel hardware architecture design based on customized dataflow and explore the reusability of input matrix. The evaluation demonstrates that Leda's geomean throughput and energy efficiency surpass Sextans and SDMA, the state-of-the-art SpMM accelerators on HBM FPGAs, and K80 GPU by 1.27x and 1.36x, 1.85x and 2.00x, 1.95x and 5.23x, respectively."

- title: "Cuper: Customized Dataflow and Perceptual Decoding for Sparse Matrix-Vector Multiplication on HBM-Equipped FPGAs"
  author: "Enxin Yi"
  institution: "China University of Petroleum-Beijing"
  link: "https://ieeexplore.ieee.org/document/10546672/"
  github: ""
  abstract: |
    "Sparse matrix-vector multiplication (SpMV) is pivotal in many scientific computing and engineering applications. Considering the memory-intensive nature and irregular data access patterns inherent in SpMV, its acceleration is typically bounded by the limited bandwidth. Multiple memory channels of the emerging high bandwidth memory (HBM) provide exceptional bandwidth, offering a great opportunity to boost the performance of SpMV. However, ensuring high bandwidth utilization with low memory access conflicts is still non-trivial. In this paper, we present Cuper, a high-performance SpMV accelerator on HBM-equipped FPGAs. Through customizing the dataflow to be HBM-compatible with the proposed sparse storage format, the bandwidth utilization can be sufficiently enhanced. Furthermore, a two-step reordering algorithm and perceptual decoder-centric hardware architecture are designed to greatly mitigate read-after-write (RAW) conflicts, enhance the vector reusability and on-chip memory utilization. The evaluation of 12 large matrices shows that Cuper's geomean throughput outperforms the four latest SpMV accelerators HiSparse, GraphLily, Sextans, and Serpens, by 3.28x, 1.99x, 1.75x, and 1.44x, respectively. Furthermore, the geomean bandwidth efficiency shows 3.28x, 2.20x, 2.82x, and 1.31x improvements, while the geomean energy efficiency has 3.59x, 2.08x, 2.21x, and 1.44x optimizations, respectively. Cuper also demonstrates 2.51x throughput and 7.97x energy efficiency of improvement over the K80 GPU on 2,757 SuiteSparse matrices."
