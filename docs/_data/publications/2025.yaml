- title: "A Unified Framework for Automated Code Transformation and Pragma Insertion"
  author: "Stéphane Pouget"
  institution: "UCLA"
  link: "https://dl.acm.org/doi/10.1145/3706628.3708873"
  abstract: |
    "High-Level Synthesis compilers and Design Space Exploration tools have greatly advanced the automation of hardware design, improving development time and performance. However, achieving a good Quality of Results still requires extensive manual code transformations, pragma insertion, and tile size selection, which are typically handled separately. The design space is too large to be fully explored by this fragmented approach. It is too difficult to navigate this way, limits the exploration of potential optimizations, and complicates the design generation process.
    To tackle this obstacle, we propose Sisyphus, a unified framework that automates code transformation, pragma insertion, and tile size selection within a common optimization framework. By leveraging Nonlinear Programming, our approach efficiently explores the vast design space of regular loop-based kernels, automatically selecting loop transformations and pragmas that minimize latency.
    Evaluation against state-of-the-art frameworks, including AutoDSE, NLP-DSE, and ScaleHLS, shows that Sisyphus achieves superior Quality of Results, outperforming alternatives across multiple benchmarks. By integrating code transformation and pragma insertion into a unified model, Sisyphus significantly reduces design generation complexity and improves performance for FPGA-based systems."

- title: "Accelerated Phylogenetics on the AMD® Versal™ Adaptive SoC"
  author: "Geert Roks"
  institution: "University of Twente"
  link: "https://dl.acm.org/doi/10.1145/3747592"
  abstract: |
    "Phylogenetics study the evolutionary history of organisms using an iterative procedure of creating and evaluating phylogenetic trees. This procedure is highly compute-intensive; constructing a large phylogenetic tree requires hundreds to thousands of CPU hours. Most phylogenetic analyses today rely either on maximum likelihood (ML) or Bayesian inference (BI) methods for inferring phylogenetic trees; the phylogenetic likelihood function (PLF) is employed in both ML and BI approaches as the tree-evaluation function, accounting for up to 95% of the overall analysis time. In this work, we explore the AMD® Versal™ Adaptive SoC architecture for accelerating the PLF that heavily relies on matrix multiplication operations. We find that the tight integration of domain-specific processors (AI Engines) with programmable logic (PL) in the Versal architecture is highly suitable for the needs of the PLF: we map the core operation of the PLF (matrix multiplication) to the AI Engines and deploy special-purpose units in the PL for PLF-specific data caching and input/output management, as well as numerical scaling that is a prerequisite for yielding numerically stable solutions for large-scale phylogenetic studies. We conducted a thorough performance analysis to leverage the platform capabilities to guide matrix-multiplication acceleration that requires close PL-AIE cooperation. We observed between 23.8x and 47.0x higher computational power of the Versal SoC than one x86 CPU core (both AMD® and Intel®) using AVX2 intrinsics, and between 3.7x and 5.9x higher performance than eight cores. For the full system, we observe comparable performance (±30%) with 8 CPU cores due to device memory access and PCIe® limitations, showing the potential of the Versal architecture in accelerating a distinct application from DSP/AI, its primary design focus."

- title: "AuroraFlow, an Easy-to-Use, Low-Latency FPGA Communication Solution Demonstrated on Multi-FPGA Neural Network Inference"
  author: "Gerrit Pape and Bjarne Wintermann"
  institution: "Paderborn University"
  link: "https://dl.acm.org/doi/10.1145/3728179.3728190"
  abstract: |
    "Communication between FPGAs has become increasingly important in recent years, with a multitude of implementations available. In this work, we focus specifically on direct FPGA-to-FPGA connections with a serial protocol. We present AuroraFlow−which is built on top of the official IP for the Aurora protocol from AMD−and use it to implement multi-FPGA neural network inference. AuroraFlow makes it easy to integrate low-latency communication into existing FPGA applications using High-Level Synthesis (HLS). It achieves full throughput, has integrated monitoring to detect errors at run-time, and implements flow control, which prevents data loss caused by receiver overruns. In a circuit-switched optical network, we are able to achieve an average latency of down to 0.51 µs and an average throughput of up to 95.03 Gbit/s for a message size of 1 MiB. To demonstrate AuroraFlow beyond microbenchmarks, we execute distributed neuronal network inference on multiple data center FPGAs. For this, we use FINN, a framework for creating dataflow hardware designs for neural network inference. We extend FINN by integrating AuroraFlow to work with multiple FPGAs. We showcase the commonly referenced MobileNet network architecture as an example where both tools can be combined: While FINN takes care of creating the inference design itself, AuroraFlow manages the communication between the FPGAs. Using AuroraFlow gives us the possibility to keep the benefit of very low inference latency that FINN enables, while allowing us to scale beyond a single FPGA."

- title: "BPAP: FPGA Design of a RISC-like Processor for Elliptic Curve Cryptography Using Task-Level Parallel Programming in High-Level Synthesis"
  author: "Rares Ifrim"
  institution: "National University of Science and Technology Politehnica Bucharest"
  link: "https://www.mdpi.com/2410-387X/9/1/20"
  github: "https://gitlab.com/raresifrim/secp256k1_ecc_pointmul"
  abstract: |
    "Popular technologies such as blockchain and zero-knowledge proof, which have already entered the enterprise space, heavily use cryptography as the core of their protocol stack. One of the most used systems in this regard is Elliptic Curve Cryptography, precisely the point multiplication operation, which provides the security assumption for all applications that use this system. As this operation is computationally intensive, one solution is to offload it to specialized accelerators to provide better throughput and increased efficiency. In this paper, we explore the use of Field Programmable Gate Arrays (FPGAs) and the High-Level Synthesis framework of AMD Vitis in designing an elliptic curve point arithmetic unit (point adder) for the secp256k1 curve. We show how task-level parallel programming and data streaming are used in designing a RISC processor-like architecture to provide pipeline parallelism and increase the throughput of the point adder unit. We also show how to efficiently use the proposed processor architecture by designing a point multiplication scheduler capable of scheduling multiple batches of elliptic curve points to utilize the point adder unit efficiently. Finally, we evaluate our design on an AMD-Xilinx Alveo-family FPGA and show that our point arithmetic processor has better throughput and frequency than related work."

- title: "Clementi: Efficient Load Balancing and Communication Overlap for Multi-FPGA Graph Processing"
  author: "Feng Yu"
  institution: "NUS"
  link: "https://dl.acm.org/doi/10.1145/3725275"
  github: "https://github.com/Xtra-Computing/Clementi"
  abstract: |
    "Efficient graph processing is critical in various modern applications, such as social network analysis, recommendation systems, and large-scale data mining. Traditional single-FPGA systems struggle to handle the increasing size and complexity of real-world graphs due to limitations in memory and computational resources. Existing multi-FPGA solutions face significant challenges, including high communication overhead caused by irregular data transfer patterns and workload imbalances stemming from skewed graph distributions. These inefficiencies hinder scalability and performance, highlighting a critical research gap. To address these issues, we introduce Clementi, an efficient multi-FPGA graph processing framework that features customized fine-grained pipelines for computation and cross-FPGA communication. Clementi uniquely integrates an accurate performance model for execution time prediction, enabling a novel scheduling method that balances workload distribution and minimizes communication overhead by overlapping communication and computation stages. Experimental results demonstrate that Clementi achieves speedups of up to 8.75x compared to state-of-the-art multi-FPGA designs, indicating significant improvements in processing efficiency as the number of FPGAs increases. This near-linear scalability underscores the framework's potential to enhance graph processing capabilities in practical applications."

- title: "Configurable DSP-Based CAM Architecture for Data-Intensive Applications on FPGAs"
  author: "Yao Chen"
  institution: "NUS"
  link: ""
  github: "https://github.com/Xtra-Computing/DSP_CAM"
  abstract: |
    "Content-addressable memory (CAM) is a type of fast memory unique in its ability to perform parallel searches of stored data based on content rather than specific memory addresses. They have been used in many domains, such as networking, databases, and graph processing. Field-programmable gate arrays (FPGAs) are an attractive platform for implementing CAMs because of their low latency, reconfigurability, and energy-efficient nature. However, such implementations also face significant challenges, including high resource utilization, limited scalability, and suboptimal performance due to the extensive use of look-up tables (LUTs) and block RAMs (BRAMs). These issues stem from the inherent limitations of FPGA architectures when handling the parallel operations required by CAMs, often leading to inefficient designs that cannot meet the demands of high-speed, data-intensive applications. To address these challenges, we propose a novel configurable CAM architecture that leverages the digital signal processing (DSP) blocks available in modern FPGAs as the core resource. By utilizing DSP blocks' data storage and logic capabilities, our approach enables configurable CAM architecture with efficient multi-query support while significantly reducing search and update latency for data-intensive applications. The DSP-based CAM architecture offers enhanced scalability, higher operating frequency, and improved performance compared to traditional LUT and BRAM-based designs. In addition, we demonstrate the effectiveness of our proposed CAM architecture with a triangle counting application on real graphs. This innovative use of DSP blocks also opens up new possibilities for high-performance, data-intensive applications on FPGAs."

- title: "Coyote v2: Raising the Level of Abstraction for Data Center FPGAs"
  author: "Benjamin Ramhorst and Maximilian J. Heer"
  institution: "ETH Zurich"
  link: "https://arxiv.org/abs/2504.21538"
  abstract: |
    "In the trend towards hardware specialization, FPGAs play a dual role as accelerators for offloading, e.g., network virtualization, and as a vehicle for prototyping and exploring hardware designs. While FPGAs offer versatility and performance, integrating them in larger systems remains challenging. Thus, recent efforts have focused on raising the level of abstraction through better interfaces and high-level programming languages. Yet, there is still quite some room for improvement. In this paper, we present Coyote v2, an open source FPGA shell built with a novel, three-layer hierarchical design supporting dynamic partial reconfiguration of services and user logic, with a unified logic interface, and high-level software abstractions such as support for multithreading and multitenancy. Experimental results indicate Coyote v2 reduces synthesis times between 15% and 20% and run-time reconfiguration times by an order of magnitude, when compared to existing systems. We also demonstrate the advantages of Coyote v2 by deploying several realistic applications, including HyperLogLog cardinality estimation, AES encryption, and neural network inference. Finally, Coyote v2 places a great deal of emphasis on integration with real systems through reusable and reconfigurable services, including a fully RoCE v2-compliant networking stack, a shared virtual memory model with the host, and a DMA engine between FPGAs and GPUs. We demonstrate these features by, e.g., seamlessly deploying an FPGA-accelerated neural network from Python."

- title: "Coyote v2: Towards Open-Source, Reusable Infrastructure and Abstractions for FPGAs"
  author: "Benjamin Ramhorst and Maximilian J. Heer"
  institution: "ETH Zurich"
  link: "https://capra.cs.cornell.edu/latte25/paper/3.pdf"
  abstract: |
    "We identify a number of requirements for future FPGA shells and describe Coyote v2, our first step towards a unified FPGA platform for cloud and datacenter acceleration, providing support similar to a conventional OS on a CPU"

- title: "Cuper: Customized Dataflow and Perceptual Decoding for Sparse Matrix-Vector Multiplication on HBM-Equipped FPGAs"
  author: "Enxin Yi"
  institution: "China University of Petroleum-Beijing"
  link: "https://ieeexplore.ieee.org/document/10546672/"
  github: ""
  abstract: |
    "Sparse matrix-vector multiplication (SpMV) is pivotal in many scientific computing and engineering applications. Considering the memory-intensive nature and irregular data access patterns inherent in SpMV, its acceleration is typically bounded by the limited bandwidth. Multiple memory channels of the emerging high bandwidth memory (HBM) provide exceptional bandwidth, offering a great opportunity to boost the performance of SpMV. However, ensuring high bandwidth utilization with low memory access conflicts is still non-trivial. In this paper, we present Cuper, a high-performance SpMV accelerator on HBM-equipped FPGAs. Through customizing the dataflow to be HBM-compatible with the proposed sparse storage format, the bandwidth utilization can be sufficiently enhanced. Furthermore, a two-step reordering algorithm and perceptual decoder-centric hardware architecture are designed to greatly mitigate read-after-write (RAW) conflicts, enhance the vector reusability and on-chip memory utilization. The evaluation of 12 large matrices shows that Cuper's geomean throughput outperforms the four latest SpMV accelerators HiSparse, GraphLily, Sextans, and Serpens, by 3.28x, 1.99x, 1.75x, and 1.44x, respectively. Furthermore, the geomean bandwidth efficiency shows 3.28x, 2.20x, 2.82x, and 1.31x improvements, while the geomean energy efficiency has 3.59x, 2.08x, 2.21x, and 1.44x optimizations, respectively. Cuper also demonstrates 2.51x throughput and 7.97x energy efficiency of improvement over the K80 GPU on 2,757 SuiteSparse matrices."

- title: "Efficient and Distributed Computation of Electron Repulsion Integrals on AMD AI Engines"
  author: "Johannes Menzel and Christian Plessl"
  institution: "Paderborn University"
  link: "https://ieeexplore.ieee.org/document/11008940"
  abstract: |
    "Computing electron repulsion integrals (ERIs) is the major computational bottleneck of many quantum mechanical simulation methods, requiring trillions of ERI evaluations per time step. While the computation of independent ERIs is embarrassingly parallel, the efficient computation of individual ERIs on modern processor cores is difficult due to both an insufficient cache size for intermediates of the computation and irregular memory access patterns that are difficult to vectorize. In this paper, we present how our implementation on the AI Engine (AIE) architecture addresses both of these problems. First, we have defined a flexible graph structure, which we call an ERI-Engine, that can be implemented for all 231 canonical ERI quartets from {ss|ss} to {hh|hh} by distributing the computation over 2-14 AIEs. Second, for the larger quartets, we have devised a novel vectorization scheme that leverages the advanced floating-point unit of the AIEs, while also supporting vectorization of independent ERIs for the smaller quartets. Finally, ERI-Engines are horizontally and vertically stackable to fill the entire AIE array, and in particular, the vertically stacked ERI-Engines form a column that uses one or more time-shared channels to stream the results out of the AIE array, almost completely hiding the computational phases of individual ERI-Engines. In terms of absolute performance, we are competitive with recent high-performance implementations of ERI algorithms on FPGAs (SERI) and GPUs (LibintX), as well as well-established highly optimized CPU libraries (Libint, Libcint), while being the unequivocal leader in terms of energy efficiency."

- title: "Evaluating the Strong Scaling Potential of AI Engines for Molecular Dynamics Simulations"
  author: "Mika Bröker"
  institution: "Paderborn University"
  link: "https://dl.acm.org/doi/10.1145/3728179.3728187"
  abstract: |
    "In molecular dynamics (MD) applications, there is a constant need to simulate ever longer timescales for a fixed number of particles. Here, novel spatial architectures such as the AI Engines (AIEs), with their low-latency interconnect between computational units, are promising candidates for breaking this strong scaling barrier. In this work, we have prototyped an MD simulation on the AIEs using cell lists that subdivide and map the simulation volume to AIEs for local computation. Between time steps, particles must be exchanged globally, for which we leveraged the on-chip switch network. We initially implemented this global communication using a ring topology, but a thorough evaluation revealed that this hindered strong scaling. Therefore, we are currently in the process of changing the global communication to a packet-switching and routing scheme to allow more direct particle exchange between AIEs. Here, we report first results and challenges posed by the architecture for an efficient implementation."

- title: "Exploring Large Language Models for Hierarchical Hardware Circuit and Testbench Generation"
  author: "Samuel Gomes Lopes"
  institution: "ETH Zurich"
  link: "https://dl.acm.org/doi/10.1145/3742430"
  abstract: |
    "Designing and verifying hardware circuits using a Hardware Description Language (HDL) is an essential but time-consuming part of hardware design. Generating the desired correct circuit and testbench code usually requires a significant engineering effort. Recently, Large Language Models (LLMs) have claimed to have strong code generation capabilities to reduce such engineering costs. Existing work has provided quantitative evaluations using LLMs for single-module, simple circuit generation. However, it is still unclear whether modern LLMs are useful in production workflows, e.g., generating correct hierarchical circuits with testbenches. And if they are capable, what are the best prompt engineering practices for hardware design? We evaluate LLMs for HDL generation by exploring a 3-dimensional design space: commercial and open-source language models, single-module and hierarchical circuits, and prompting methods with varying complexity. We propose a 3-step design space exploration methodology to answer the two aforementioned questions. First, we explore the best prompt engineering practices across generating simple, middle, and hard single-module circuits with testbenches on CodeLLama-34B. We also define two fine-grained checklists to evaluate the circuit and testbench quality from a user's perspective. Second, we benchmark 11 LLMs with prompt adaptation on 4 single-module circuits that CodeLLama-34B has trouble with to further find models that may be useful in a production workflow. Third, we apply the learned prompt practices on four top-level models to generate simple 2 to 4-module and more complex multi-module hierarchical circuits and testbenches. As a result, we find that some of the latest LLMs can generate correct simple hierarchical circuits and testbenches with given proper prompts, but still struggle with complex hierarchical circuits. We further provide useful guidelines from an end-user's perspective on leveraging LLMs for hardware design."

- title: "Fast Graph Vector Search via Hardware Acceleration and Delayed-Synchronization Traversal"
  author: "Wenqi Jiang"
  institution: "ETH Zurich"
  link: "https://dl.acm.org/doi/abs/10.14778/3749646.3749655"
  abstract: |
    "Vector search systems are indispensable in large language model (LLM) serving, search engines, and recommender systems, where minimizing online search latency is essential. Among various algorithms, graph-based vector search (GVS) is particularly popular due to its high search performance and quality. However, reducing GVS latency by intra-query parallelization remains challenging due to limitations imposed by both existing hardware architectures (CPUs and GPUs) and the inherent difficulty of parallelizing graph traversals. To efficiently serve low-latency GVS, we co-design hardware and algorithm by proposing Falcon and Delayed-Synchronization Traversal (DST). Falcon is a hardware GVS accelerator that implements efficient GVS operators, pipelines these operators, and reduces memory accesses by tracking search states with an on-chip Bloom filter. DST is an efficient graph traversal algorithm that simultaneously improves search performance and quality by relaxing traversal orders to maximize accelerator utilization. Evaluation across various graphs and datasets shows that Falcon, prototyped on FPGAs, together with DST, achieves up to 4.3X and 19.5X lower latency and up to 8.0X and 26.9X improvements in energy efficiency over CPU- and GPU-based GVS systems."

- title: "FINN-HPC: Closing the Gap for Energy-Efficient Neural Network Inference on FPGAs in HPC"
  author: "Linus Jungemann"
  institution: "Paderborn University"
  link: "https://dl.acm.org/doi/10.1145/3728179.3728189"
  abstract: |
    "In recent years, neural network (NN) training and inference have become one of the most impactful topics in society, research and industry. With the rise in popularity of NNs and the increase in their capabilities and size, new challenges surface, such as a high energy consumption. This paper presents a comprehensive evaluation of current NN inference soft- and hardware configurations within High-Performance Computing (HPC) environments, with a focus on both performance metrics and energy consumption. NN quantization and the use of accelerators such as FPGAs can improve throughput and energy efficiency. This paper focuses on FINN, an efficient NN inference framework for FPGAs, highlighting its current lack of support for HPC systems. We provide an in-depth analysis of FINN in order to implement extensions and a new user-space driver for FINN to optimize the end-to-end execution for the usage in the HPC environment. We thoroughly evaluate the performance and energy efficiency gains using the newly implemented optimizations for a NN requiring low latency and high throughput, as well as for a transformer model. The results are compared against existing NN accelerators for HPC. Our results show that with our newly developed FINN HPC driver, FPGAs have a real use case depending on input sizes and network. For the simpler model, we show that for smaller input sizes, FPGAs with our driver can provide up to 7.81× higher throughput and 88% lower energy consumption over the GPU reference, which however still performs better for larger input sizes. Additionally, we evaluate the newly implemented FINN HPC driver on the significantly more complex transformer based model. We show that for smaller input sizes, our optimizations results in an up 6.51× higher throughput, while at the same time reducing energy consumption by 87% and latency by 85%."

- title: "HBMex: An Attachment for Nonbursting Accelerators to Enhance HBM Performance"
  author: "Canberk Sönmez"
  institution: "EPFL"
  link: "https://ieeexplore.ieee.org/abstract/document/11008972"
  abstract: |
    "Modern FPGAs integrate High Bandwidth Memory (HBM), offering up to 12× the DDR bandwidth distributed across multiple memory interfaces. To utilize the most of HBM's theoretical bandwidth, accelerators typically issue long bursts and exploit data locality. However, some applications like sparse matrix-vector multiplication (SpMV) and graph analytics often exhibit irregular, nonbursting memory access patterns, which hinder performance. Additionally, the HBM interconnect, essential for accessing multiple interfaces, may stall requests under certain conditions. This work introduces HBMex, a novel module designed to enhance HBM throughput for accelerators with irregular access patterns. Positioned between the accelerator and the HBM, HBMex improves parallelism by distributing memory requests across interfaces and mitigates stalls caused by the interconnect. We evaluate HBMex using memory access microbenchmarks and an SpMV accelerator, demonstrating throughput gains of up to 37% across real-world workloads compared to vendor-provided solutions. HBMex is distributed as a highly-configurable and open-source RTL generator."

- title: "Iceberg: Enhancing HLS Modeling with Synthetic Data"
  author: "Zijian Ding"
  institution: "UCLA"
  link: ""
  github: "https://github.com/UCLA-VAST/iceberg"
  abstract: |
    "Deep learning-based prediction models for High-Level Synthesis (HLS) of hardware designs often struggle to generalize. In this paper, we study how to close the generalizability gap of these models through pretraining on synthetic data and introduce Iceberg, a synthetic data augmentation approach that expands both large language model (LLM)-generated programs and weak labels of unseen design configurations. Our weak label generation method is integrated with an in-context model architecture, enabling meta-learning from actual and proximate labels. Iceberg improves the geometric mean modeling accuracy by 86.4% when adapt to six real-world applications with few-shot examples and achieves a 2.47× and a 1.12× better offline DSE performance when adapting to two different test datasets."

- title: "InTAR: Inter-Task Auto-Reconfigurable Accelerator Design for High Data Volume Variation in DNNs"
  author: "Zifan He"
  institution: "UCLA"
  link: "https://ieeexplore.ieee.org/abstract/document/11008978"
  github: "https://github.com/OswaldHe/lnTAR"
  abstract: |
    "The rise of deep neural networks (DNNs) has driven an increased demand for computing power and memory. Modern DNNs exhibit high data volume variation (HDV) across tasks, which poses challenges for FPGA acceleration: conventional accelerators rely on fixed execution patterns (dataflow or sequential) that can lead to pipeline stalls or necessitate frequent off-chip memory accesses. To address these challenges, we introduce the Inter-Task Auto-Reconfigurable Accelerator (InTAR), a novel accelerator design methodology for HDV applications on FPGAs. InTAR combines the high computational efficiency of sequential execution with the reduced off-chip memory overhead of dataflow execution. It switches execution patterns automatically with a static schedule determined before circuit design based on resource constraints and problem sizes. Unlike previous reconfigurable accelerators, InTAR encodes reconfiguration schedules during circuit design, allowing model-specific optimizations that allocate only the necessary logic and interconnects. Thus, InTAR achieves a high clock frequency with fewer resources and low reconfiguration time. Furthermore, InTAR supports high-level tools such as HLS for fast design generation. We implement a set of multi-task HDV DNN kernels using InTAR. Compared with dataflow and sequential accelerators, InTAR exhibits 1.8× and 7.1× speedups correspondingly. Moreover, we extend InTAR to GPT-2 medium as a more complex example, which is 3.65 ~39.14× faster and a 1.72 ~ 10.44× more DSP efficient than SoTA accelerators (Allo and DFX) on FPGAs. Additionally, this design demonstrates 1.66 ~ 7.17× better power efficiency than GPUs."

- title: "Leda: Leveraging Tiling Dataflow to Accelerate SpMM on HBM-Equipped FPGAs for GNNs"
  author: "Enxin Yi"
  institution: "China University of Petroleum-Beijing"
  link: "https://dl.acm.org/doi/10.1145/3676536.3676773"
  github: ""
  abstract: |
    "Graph neural networks (GNNs) play a pivotal role in extracting insightful representations from graph-structured data, driving advancements across diverse domains. Central to GNNs is the sparse matrix-dense matrix multiplication (SpMM) kernel. However, challenges arise in accelerating SpMM due to the high sparsity and randomly distributed non-zeros in graph matrices. Recently, the high concurrency capability of high bandwidth memory (HBM) has provided a new opportunity for SpMM acceleration. Nonetheless, accelerating SpMM on HBM FPGAs is still non-trivial due to load imbalance and the random memory access patterns. In this paper, we present Leda, a high-performance SpMM accelerator on HBM-equipped FPGAs for GNNs. Leda utilizes a tiling sparse format to balance the workload between processing engines (PEs) and enhance data locality. The improved outer-product dataflow scheduling strategy mitigates the random memory access bottlenecks. Furthermore, we introduce a minimum similarity reordering algorithm to significantly optimize read-after-write (RAW) dependencies, thereby improving compute occupancy and throughput. Finally, we propose a highly parallel hardware architecture design based on customized dataflow and explore the reusability of input matrix. The evaluation demonstrates that Leda's geomean throughput and energy efficiency surpass Sextans and SDMA, the state-of-the-art SpMM accelerators on HBM FPGAs, and K80 GPU by 1.27x and 1.36x, 1.85x and 2.00x, 1.95x and 5.23x, respectively."

- title: "LLM Acceleration on FPGAs: A Comparative Study of Layer and Spatial Accelerators"
  author: "Luis D. Prieto-Sibaja"
  institution: "Costa Rica Institute of Technology"
  link: "https://ieeexplore.ieee.org/document/10933896"
  abstract: |
    "Large Language Models (LLMs) have emerged as the leading technique in Natural Language Processing due to their remarkable capabilities. These models are exceptionally complex, often utilising billions of parameters and consuming tens of gigabytes of memory unless optimised. Typically, LLMs are trained and accelerated using general-purpose Graphics Processing Units (GPUs), with a single inference potentially occupying an entire GPU. Consequently, cloud services may require hundreds of GPUs to deliver high-quality AI assistant services. This demand for extensive hardware acceleration allows exploring alternative architectures that offer improved execution time and energy efficiency.This study evaluates Field-Programmable Gate Arrays (FP-GAs), renowned for their flexibility and efficiency, to accelerate LLMs. We specifically compare the resource consumption and latency of two distinct architectures: layer and spatial accelerators. Analysing the Llama 2-7B model, we identified potential optimisation opportunities within its composition and operational graphs. Our most successful implementations demonstrate a performance improvement ranging from 1.37× to 10.98× over two AMD EPYC processors with 64 cores each. Moreover, our results indicate that, with further refinement, FPGAs have the potential to surpass GPU performance for LLM inference, showcasing their feasibility as a viable alternative for this demanding application."

- title: "Machine Learning-based Deep Packet Inspection at Line Rate for RDMA on FPGAs"
  author: "Maximilian Heer, Benjamin Ramhorst"
  institution: "ETH Zurich"
  link: "https://dl.acm.org/doi/pdf/10.1145/3721146.3721935"
  abstract: |
    "FPGAs are becoming increasingly important in the cloud and data centers, especially as network-attached accelerators or reconfigurable Network Interface Cards (NICs). In the cloud, Remote Direct Memory Access (RDMA) over Converged Ethernet (RoCEv2) has emerged as the de facto standard protocol for data transport due to its low latency and high throughput. However, RDMA has several access control weaknesses limiting its applicability in the cloud. In this paper, we explore using machine learning-based deep packet inspection (DPI) as an enhancement to an open-source FPGA RDMA stack. The ultra low-latency ML model is integrated on the RDMA datapath and allows for detection of specific content in RDMA payloads (e.g., executables) at a line rate of 100Gbps while using less than 1% of the available resources. Compared with existing work, our solution operates on the full message payload, at the transport level, and on a complete RDMA stack without sacrificing compatibility with RoCEv2 and its native performance characteristics, proving its potential as an end-to-end solution."

- title: "Neural Network Inference in High-Performance Computing: Closing the Gap for FINN based Reconfigurable Accelerators"
  author: "Linus Jungemann"
  institution: "Paderborn University"
  link: "https://dl.acm.org/doi/abs/10.1145/3706628.3708857"
  abstract: |
    "In recent years, Neural Networks (NNs) have become one of the most prevailing topics in computers science, both in research and in industry. NNs are used for data analysis, natural language processing, autonomous driving and more. As such, NNs also see more application and use in High-Performance Computing (HPC). At the same time, energy efficiency has become an increasingly critical topic. NNs use large amounts of energy for operation, which in return results in large amounts of CO2 emissions. This work presents a comprehensive evaluation of current NN inference soft- and hardware configurations within High-Performance Computing (HPC) environments, with a focus on both performance metrics and energy consumption. NN quantization and accelerators such as FPGAs allow for an increased inference efficiency, both in terms of throughput and energy. Therefore, this work focuses on FINN, an efficient NN inference framework for FPGAs, highlighting its current lack of support for HPC systems. We provide an in-depth analysis of FINN in order to implement extensions to optimize the end-to-end execution for the usage in the HPC environment. We thoroughly evaluate the performance and energy efficiency gains using newly implemented optimizations and compare it against existing NN accelerators for HPC. With our extensions of FINN, we were able to achieve a 1847× higher throughput, while also decreasing the latency on average by 0.9978× and EDP by 0.9979× on an Alveo U55C FPGA. Data flow based NN inference accelerators on an FPGA should be used if the performance and energy footprint of the inference process is crucial, and the batch sizes are small to medium. For extremely large batch sizes and a very limited time for network-to-accelerator (less than a few days), using GPUs is still the way to go. Our results show that with the newly developed driver, we outperform a high-end Nvidia A100 GPU by up to 7.81x in throughput, while having a 0.87x lower latency and 0.88x lower energy delay product."

- title: "OpenRTLSet: A Fully Open-Source Dataset for Large Language Model-based Verilog Module Design"
  author: "Jinghua Wang"
  institution: "UIUC"
  link: "https://ieeexplore.ieee.org/abstract/document/11106163"
  abstract: |
    "OpenRTLSet1 introduces the largest fully open-source dataset for hardware design, offering over 127,000 diverse Verilog code samples to the research community and industry. Our dataset uniquely combines Verilog code from GitHub repositories (98k modules), VHDL translations (5k modules), and synthesizable C/C++ translations (24k modules), all freely accessible without proprietary restrictions. Using the reasoning model DeepSeek-R1, we generated paired natural language descriptions for each code sample, enabling fine-tuning of various language model families (e.g., Qwen and Granite) for Verilog code generation. Our dataset explores multiple options, including Verilator-generated C++ files as additional context during labeling, quantization techniques (INT4 vs. BF16), and performance differences across model sizes (7B-32B parameters). OpenRTLSet demonstrates that open-source approaches can achieve superior performance in hardware design tasks, establishing a new foundation for accessible research and commercial use in this domain."

- title: "Parallel Accurate Minifloat MACCs for Neural Network Inference on Versal FPGAs"
  author: "Hans Jakob Damsgaard"
  institution: "Tampere University"
  link: "https://ieeexplore.ieee.org/document/10777058"
  abstract: |
    "Machine learning (ML) is ubiquitous in contemporary applications. Its need for efficient acceleration has driven vast research efforts into the quantization of neural networks with low-precision numerical formats. Models quantized with minifloat formats of eight or fewer bits have proven capable of outperforming models quantized into same-size integers. However, unlike integers, minifloats require accurate accumulation to prevent the introduction of rounding errors. We explore the design space of parallel accurate minifloat multiply-accumulators (MACCs) targeting the AMD VersalTM FPGA fabric. We experiment with three variations of the multiply-and-shift and adder tree components of a minifloat MACC. For comparison, we apply similar alterations to a parallel integer MACC. Our results show that custom compressor trees with external sign-inversion gates reduce the mean area of the minifloat MACCs by 17.7% and increase their clock frequency by 16.2%. In comparison, custom compressor trees with absorbed partial product generation gates reduce the mean area of integer MACCs by 28.1% and increase their clock frequency by 3.60%. Comparing the best-performing designs, we observe that minifloat MACCs consume 20% to 180% more resources than integer ones with same-size operands without accounting for a conversion back into a floating-point format, and 60% to 300% more resources when including it. Our data enable engineers to make informed decisions in their designs of deeply integrated embedded ML solutions when trading off training and fine-tuning effort versus resource cost."

- title: "Performance-Portable implementation of the shallow water equations on CPUs, GPUs, and FPGAs"
  author: "Markus Büttner"
  institution: "Paderborn University"
  link: "https://dl.acm.org/doi/abs/10.1145/3731125.3731134"
  abstract: |
    "The shallow water equations describe fluid flows where the horizontal length scales are much larger than the vertical scales. Applications for the shallow water equations can be found in the modelling of tides, estuaries, tsunamis, floods, or atmospheric flows. We present an implementation of the shallow water equations for coastal ocean domains, running on CPUs, GPUs, and FPGAs by utilizing SYCL as a platform-agnostic programming model."

- title: "Reconfigurable Stream Network Architecture"
  author: "Chengyue Wang"
  institution: "UCLA"
  link: "https://dl.acm.org/doi/full/10.1145/3695053.3731088"
  github: "https://github.com/ChengyueWang/ISCA25-Stream-Network-Arch"
  abstract: |
    "As AI systems grow increasingly specialized and complex, managing hardware heterogeneity becomes a pressing challenge. How can we efficiently coordinate and synchronize heterogeneous hardware resources to achieve high utilization? How can we minimize the friction of transitioning between diverse computation phases, reducing costly stalls from initialization, pipeline setup, or drain? Our insight is that a network abstraction at the ISA level naturally unifies heterogeneous resource orchestration and phase transitions. 
    This paper presents a Reconfigurable Stream Network Architecture (RSN), a novel ISA abstraction designed for the DNN domain. RSN models the datapath as a circuit-switched network with stateful functional units as nodes and data streaming on the edges. Programming a computation corresponds to triggering a path. Software is explicitly exposed to the compute and communication latency of each functional unit, enabling precise control over data movement for optimizations such as compute-communication overlap and layer fusion. As nodes in a network naturally differ, the RSN abstraction can efficiently virtualize heterogeneous hardware resources by separating control from the data plane, enabling low instruction-level intervention.
    We build a proof-of-concept design RSN-XNN on VCK190, a heterogeneous platform with FPGA fabric and AI engines. Compared to the SOTA solution on this platform, it reduces latency by 6.1x and improves throughput by 2.4x–3.2x. Compared to the T4 GPU with the same FP32 performance, it matches latency with only 18% of the memory bandwidth. Compared to the A100 GPU at the same 7nm process node, it achieves 2.1x higher energy efficiency in FP32."

- title: "RoCE BALBOA: Towards FPGA-enhanced RDMA"
  author: "Maximilian J. Heer"
  institution: "ETH Zurich"
  link: "https://2025.eurosys.org/posters/final/eurosys25posters-final84.pdf"
  abstract: |
    "We explore the use of FPGA-based SmartNICs in the context of capability-enhanced RDMA, by combining a self-developed, open-source and fully RoCE v2-compatible networking stack with exemplaric hardware modules for AESencryption, snappy-compression, and machine learning-based Deep Packet Inspection (DPI) for access control that utilize stream computing to achieve 100G line rate speed without any performance overhead on the host CPU (Figure 1). Furthermore, our design allows to add any other user-defined application directly on the RDMA data streams, leveraging the reconfigurable fabric for offloading network processing from the host CPU to the FPGA-NIC at full line rate."

- title: "RoCE BALBOA: Service-enhanced Data Center RDMA for SmartNICs"
  author: "Maximilian J. Heer"
  institution: "ETH Zurich"
  link: "https://arxiv.org/abs/2507.20412"
  abstract: |
    "Data-intensive applications in data centers, especially machine learning (ML), have made the network a bottleneck, which in turn has motivated the development of more efficient network protocols and infrastructure. For instance, remote direct memory access (RDMA) has become the standard protocol for data transport in the cloud as it minimizes data copies and reduces CPU-utilization via host-bypassing. Similarly, an increasing amount of network functions and infrastructure have moved to accelerators, SmartNICs, and in-network computing to bypass the CPU. In this paper we explore the implementation and deployment of RoCE BALBOA, an open-source, RoCE v2-compatible, scalable up to hundred queue-pairs, and 100G-capable RDMA-stack that can be used as the basis for building accelerators and smartNICs. RoCE BALBOA is customizable, opening up a design space and offering a degree of adaptability not available in commercial products. We have deployed BALBOA in a cluster using FPGAs and show that it has latency and performance characteristics comparable to commercial NICs. We demonstrate its potential by exploring two classes of use cases. One involves enhancements to the protocol for infrastructure purposes (encryption, deep packet inspection using ML). The other showcases the ability to perform line-rate compute offloads with deep pipelines by implementing commercial data preprocessing pipelines for recommender systems that process the data as it arrives from the network before transferring it directly to the GPU. These examples demonstrate how BALBOA enables the exploration and development of SmartNICs and accelerators operating on network data streams."

- title: "Rock the QASBA: Quantum Error Correction Acceleration via the Sparse Blossom Algorithm on FPGAs"
  author: "Marco Venere"
  institution: "Politecnico di Milano"
  link: "https://dl.acm.org/doi/10.1145/3723168"
  abstract: |
    "Quantum computing is a new paradigm of computation that exploits principles from quantum mechanics to achieve an exponential speedup compared to classical logic. However, noise strongly limits current quantum hardware, reducing achievable performance and limiting the scaling of the applications. For this reason, current noisy intermediate-scale quantum devices requireQuantum Error Correction (QEC) mechanisms to identify errors occurring in the computation and correct them in real time. Nevertheless, the high computational complexity of QEC algorithms is incompatible with the tight time constraints of quantum devices. Thus, hardware acceleration is paramount to achieving real-time QEC. This work presents QASBA, an FPGA-based hardware accelerator for the Sparse Blossom Algorithm (SBA), a state-of-the-art decoding algorithm. After profiling the state-of-the-art software counterpart, we developed a design methodology for hardware development based on the SBA. We also devised an automation process to help users without expertise in hardware design in deploying architectures based on QASBA. We implement QASBA on different FPGA architectures and experimentally evaluate resource usage, execution time, and energy efficiency of our solution. Our solution attains up to 25.05× speedup and 304.16× improvement in energy efficiency compared to the software baseline."

- title: "SAT-Accel: A Modern SAT Solver on a FPGA"
  author: "Michael Lo"
  institution: "UCLA"
  link: "https://dl.acm.org/doi/10.1145/3706628.3708869"
  abstract: |
    "Boolean satisfiability (SAT) solving is the first known NP-complete problem and is widely used in many application domains. Over the years, there have been so many consistent improvements in this area such that larger instances can be solved relatively quickly. Although these improvements have found their way onto CPU implementations, there has been limited progress adopting this on hardware accelerators mainly because it is difficult to implement the dynamic data structures needed to support a modern SAT solving algorithm. In this work, we present SAT-Accel, an algorithm-hardware co-design solver that applies many of the core improvements found in modern SAT solvers. SAT-Accel uses a novel memory management system and representation that supports the dynamic data structures required by a modern SAT solving algorithm. Our design can achieve on average a 17.9x speedup against MiniSat, the previous state of the art CPU solver, and a 2.8x speedup against Kissat, the current state of the art CPU solver. Compared to the current state-of-the-art stand-alone hardware accelerator, SAT-Hard, SAT-Accel achieves on average 800.0x speedup"


- title: "Self-Validated Learning for Particle Separation: A Correctness-Based Self-Training Framework Without Human Labels"
  author: "Philipp D. Lösel"
  institution: "Australian National University"
  link: "https://search.arxiv.org/paper.jsp?r=2508.16224"
  abstract: |
    "Non-destructive 3D imaging of large multi-particulate samples is essential for quantifying particle-level properties, such as size, shape, and spatial distribution, across applications in mining, materials science, and geology. However, accurate instance segmentation of particles in tomographic data remains challenging due to high morphological variability and frequent particle contact, which limit the effectiveness of classical methods like watershed algorithms. While supervised deep learning approaches offer improved performance, they rely on extensive annotated datasets that are labor-intensive, error-prone, and difficult to scale. In this work, we propose self-validated learning, a novel self-training framework for particle instance segmentation that eliminates the need for manual annotations. Our method leverages implicit boundary detection and iteratively refines the training set by identifying particles that can be consistently matched across reshuffled scans of the same sample. This self-validation mechanism mitigates the impact of noisy pseudo-labels, enabling robust learning from unlabeled data. After just three iterations, our approach accurately segments over 97% of the total particle volume and identifies more than 54,000 individual particles in tomographic scans of quartz fragments. Importantly, the framework also enables fully autonomous model evaluation without the need for ground truth annotations, as confirmed through comparisons with state-of-the-art instance segmentation techniques. "


- title: "Stream-HLS: Towards Automatic Dataflow Acceleration"
  author: "Suhail Basalama"
  institution: "UCLA"
  link: "https://dl.acm.org/doi/10.1145/3706628.3708878"
  github: "https://github.com/UCLA-VAST/Stream-HLS"
  abstract: |
    "High-level synthesis (HLS) has enabled the rapid development of custom hardware circuits for many software applications. However, developing high-performance hardware circuits using HLS is still a non-trivial task requiring expertise in hardware design. Further, the hardware design space, especially for multi-kernel applications, grows exponentially. Therefore, several HLS automation and abstraction frameworks have been proposed recently, but many issues remain unresolved. These issues include: 1) relying mainly on hardware directives (pragmas) to apply hardware optimizations without exploring loop scheduling opportunities. 2) targeting single-kernel applications only. 3) lacking automatic and/or global design space exploration. 4) missing critical hardware optimizations, such as graph-level pipelining for multi-kernel applications.
    To address these challenges, we propose a novel methodology and framework on top of the popular multi-level intermediate representation (MLIR) infrastructure called Stream-HLS. Our framework takes a C/C++ or PyTorch software code and automatically generates an optimized dataflow architecture along with host code for field-programmable gate arrays (FPGAs). To achieve this, we developed an accurate analytical performance model for global scheduling and optimization of dataflow architectures. Stream-HLS is evaluated using various standard HLS benchmarks and real-world benchmarks from transformer models, convolution neural networks, and multilayer perceptrons. Stream-HLS designs outperform the designs of prior state-of-the-art automation frameworks and manually-optimized designs of abstraction frameworks by up to 79.43× and 10.62× geometric means respectively"

- title: "SwiftSpatial: Spatial Joins on Modern Hardware"
  author: "Wenqi Jiang"
  institution: "ETH Zurich"
  link: "https://dl.acm.org/doi/abs/10.1145/3725361"
  abstract: |
    "Spatial joins are among the most time-consuming spatial queries, remaining costly even in parallel and distributed systems. In this paper, we explore hardware acceleration for spatial joins by proposing SwiftSpatial, an FPGA-based accelerator that can be deployed in data centers and at the edge. SwiftSpatial contains multiple high-performance join units with innovative hybrid parallelism, several efficient memory management units, and an extensible on-chip join scheduler that supports the popular R-tree synchronous traversal and partition-based spatial-merge (PBSM) algorithms. Benchmarked against various CPU and GPU-based spatial data processing systems, SwiftSpatial demonstrates a latency reduction of up to 41.03x relative to the best-performing baseline, while requiring 6.16x less power. The performance and energy efficiency of SwiftSpatial demonstrate its potential to be used in a variety of configurations (e.g., as an accelerator, near storage, in-network) as well as on different devices (e.g., data centers where FPGAs are widely available or mobile devices, which also contain FPGAs for specialized processing)."

- title: "VOTED: Versal optimization toolkit for education and heterogeneous systems development"
  author: "Giuseppe Sorrentino"
  institution: "Politecnico di Milano"
  link: ""
  github: "https://github.com/necst/voted"
  abstract: |
    "Despite classic educational approaches proving their effectiveness for classic hardware acceleration, they suffer novel heterogeneous systems such as Versal, requiring a deeper system-level awareness. Versal devices merge FPGA on-field programmability with the performance of hardened VLIW processors, namely AI Engine, at the cost of facing novel challenges when integrating these two different layers. Given the system complexity of such devices and the low-level knowledge required to leverage them, Versal potentialities have yet to be fully exploited. Therefore, we present VOTED, a Versal Optimization Toolkit for Education and Heterogeneous Systems Development, that guides users of any expertise in discovering, using, and optimizing Versal-based applications. VOTED proved to be effective, leading five different groups of students, at their first experience with heterogeneous system design, at devising quite complex Versal-based applications capable of reaching the final stages of a design competition and even winning it with four months of work"
